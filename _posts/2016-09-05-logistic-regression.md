---
layout: post
title: 机器学习之逻辑回归
date: 2016-09-05 20:16:18
category: "machine learning"
---


最近都在学习逻辑回归,看了不少博客的文章,有详细的,有直接上来就是贴公式的,很暴力。

不管怎样,还是要感谢博主们贡献出来的文章,也从中收获了不少。

自己学习后,把自己理解的整理归纳下,所以有了这篇文章,希望能帮助到正在学习逻辑回归的同学吧。

如果哪里写的有问题,或者有疑问,请指出留言,感谢。


### 基本概念

回归是由英国著名生物学家兼统计学家高尔顿(Francis Galton,1822～1911)在研究人类遗传问题时提出来的。为了研究父代与子代身高的关系，高尔顿搜集了1078对父亲及其儿子的身高数据。他发现这些数据的散点图大致呈直线状态，也就是说，总的趋势是父亲的身高增加时，儿子的身高也倾向于增加。但是，高尔顿对试验数据进行了深入的分析，发现了一个很有趣的现象—回归效应。因为当父亲高于平均身高时，他们的儿子身高比他更高的概率要小于比他更矮的概率；父亲矮于平均身高时，他们的儿子身高比他更矮的概率要小于比他更高的概率。它反映了一个规律，即这两种身高父亲的儿子的身高，有向他们父辈的平均身高回归的趋势。对于这个一般结论的解释是:大自然具有一种约束力，使人类身高的分布相对稳定而不产生两极分化，这就是所谓的回归效应。

Logistic Regression，逻辑回归，简称LR，是机器学习领域一种常用的算法或模型，属于Supervised Learning监督学习，是指对已知数据(历史数据等)进行训练，产生一个推断的功能。

LR是一种分类算法。也就是把一个群体分类别，可以是具体的问题，已知的数据等。比如：男人/女人，今天会下雨/今天不会下雨，广告是否点击等。

机器学习的主旨就是通过对历史数据的计算(学习)，得到一些未知的参数，从而可以判断出新数据会有什么结论。比如：y ＝ ax ＋ b，以下是已知的几组(x,y)历史数据

| x | y|
|---|---|
|1|5.5|
|1.5|6.0|
|2|6.5|

在已知历史数据的情况下，怎么预测自变量x ＝3 ，因变量y ＝ ？

求因变量y的问题，就转换为计算出两个未知参数a和b的值，有了这两个值，给定任意一个x，都能通过y = ax + b计算出y的值，这就是预测。

LR差不多也是这样，有一个函数 y ＝ f(X)，里面包含了N个未知参数ɵ0，ɵ1，ɵ2，... ɵn。因变量y通常会跟很多自变量x有关系，既x0，x1，x2，...xn，所以自变量是一个向量，用大写X表示。同样的，那一堆未知参数也是一个向量，用字母ɵ来表示。

ɵTx，这是参数向量和自变量向量的[点积](https://zh.wikipedia.org/wiki/%E6%95%B0%E9%87%8F%E7%A7%AF){:target="_blank"}，表达的含义：计算某个事件发生的可能性，可以把这个事件相关的所有特征加权求和。比如，要求今天下雨的可能性，可以把今天所有和下雨相关的概率加权求和，例如有台风经过权重为6，清明时期权重为9等等其他特征，每一个特征都影响"今天下雨的可能性"，用数学表达式表达:

![点积](/images/posts/未知参数和自变量点积.jpg)

这个加权求和的结果是在(－∞，＋∞)范围内的，为了能表示预测的概率，要把输出值在(0，1)之间。所以，逻辑函数就出现了。


### [逻辑函数](https://zh.wikipedia.org/wiki/%E9%82%8F%E8%BC%AF%E5%87%BD%E6%95%B8){:target="_blank"}

![逻辑函数](/images/posts/逻辑函数.jpg)


原创文章转载请注明出处：[快学scala笔记——对象](http://9leg.com/scala/2016/09/05/logistic-regression.html)